{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqHqr+mgMg0htluTEYuEl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablolake/pacman/blob/main/tfg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rlcard\n",
        "!pip install rlcard.utils"
      ],
      "metadata": {
        "id": "uvRtIQIKTu-R",
        "outputId": "cf2e5548-fd95-41dd-a5e0-f96a8f54f959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rlcard\n",
            "  Downloading rlcard-1.1.0.tar.gz (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.5/265.5 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.9/dist-packages (from rlcard) (1.22.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from rlcard) (2.2.0)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.1.0-py3-none-any.whl size=322200 sha256=942fd5f22fedfb48631d16ebf30d3bd1c477dd4fb0c6882543fb8c3f218ed017\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/a8/66/5a7a7be9d7d0607ec8cabde164f3b44694800f394736ce56a4\n",
            "Successfully built rlcard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPR8r6ZRTBrQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set up the game environment\n",
        "import rlcard\n",
        "from rlcard.agents import NolimitholdemHumanAgent as HumanAgent\n",
        "from rlcard.envs import Env\n",
        "from rlcard.games.nolimitholdem import Game\n",
        "from rlcard.games.nolimitholdem.round import Action\n",
        "import tensorflow as tf\n",
        "from rlcard.utils import print_card\n",
        "from rlcard.utils import (\n",
        "    get_device,\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    reorganize,\n",
        "    Logger,\n",
        "    plot_curve,\n",
        ")\n",
        "from collections import namedtuple\n",
        "from rlcard.agents.dqn_agent import DQNAgent\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done', 'legal_actions'])\n",
        "# Set up the game environment\n",
        "env = rlcard.make('no-limit-holdem')\n",
        "\n",
        "# Create the human agent object\n",
        "human_agent = HumanAgent(env.num_actions)\n",
        "\n",
        "replay_memory_size = 20000\n",
        "replay_memory_init_size = 1000\n",
        "update_target_estimator_every = 1000\n",
        "discount_factor = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.1\n",
        "epsilon_decay_steps = 20000\n",
        "batch_size = 32\n",
        "num_actions = env.num_actions\n",
        "state_shape=env.state_shape[0]\n",
        "train_every = 1\n",
        "mlp_layers = [128, 128]\n",
        "learning_rate = 0.001\n",
        "device = get_device()\n",
        "\n",
        "# Create the DQN agent object\n",
        "dqn_agent = DQNAgent(\n",
        "    replay_memory_size=replay_memory_size,\n",
        "    replay_memory_init_size=replay_memory_init_size,\n",
        "    update_target_estimator_every=update_target_estimator_every,\n",
        "    discount_factor=discount_factor,\n",
        "    epsilon_start=epsilon_start,\n",
        "    epsilon_end=epsilon_end,\n",
        "    epsilon_decay_steps=epsilon_decay_steps,\n",
        "    batch_size=batch_size,\n",
        "    num_actions=num_actions,\n",
        "    state_shape=state_shape,\n",
        "    train_every=train_every,\n",
        "    mlp_layers=mlp_layers,\n",
        "    learning_rate=learning_rate,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# Reset the environment\n",
        "state, _ = env.reset()\n",
        "\n",
        "env.set_agents([human_agent, dqn_agent])\n",
        "\n",
        "\n",
        "while (True):\n",
        "    print(\">> Start a new game\")\n",
        "\n",
        "    trajectories, payoffs = env.run(is_training=False)\n",
        "    # If the human does not take the final action, we need to\n",
        "    # print other players action\n",
        "    final_state = trajectories[0][-1]\n",
        "    action_record = final_state['action_record']\n",
        "    state = final_state['raw_obs']\n",
        "    _action_list = []\n",
        "    for i in range(1, len(action_record)+1):\n",
        "        if action_record[-i][0] == state['current_player']:\n",
        "            break\n",
        "        _action_list.insert(0, action_record[-i])\n",
        "    for pair in _action_list:\n",
        "        print('>> Player', pair[0], 'chooses', pair[1])\n",
        "\n",
        "    # Let's take a look at what the agent card is\n",
        "    print('===============     Cards all Players    ===============')\n",
        "    for hands in env.get_perfect_information()['hand_cards']:\n",
        "        print(\"hola\",num_actions)\n",
        "        print_card(hands)\n",
        "\n",
        "    print('===============     Result     ===============')\n",
        "    if payoffs[0] > 0:\n",
        "        print('You win {} chips!'.format(payoffs[0]))\n",
        "    elif payoffs[0] == 0:\n",
        "        print('It is a tie.')\n",
        "    else:\n",
        "        print('You lose {} chips!'.format(-payoffs[0]))\n",
        "    print('')\n",
        "\n",
        "    input(\"Press any key to continue...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "xRbhH3ytqAzy"
      }
    }
  ]
}